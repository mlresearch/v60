---
title: SCOT Approximation, Training and Asymptotic Inference
abstract: "Approximation of stationary strongly mixing processes by Stochastic Context
  Trees (SCOT) models\r and the Le Cam-Hajek-Ibragimov-Khasminsky locally minimax
  theory of statistical inference for them is outlined.\r SCOT is an $m$-Markov model
  with sparse memory structure.\r In our previous papers we proved SCOT equivalence
  to 1-MC with state spaceâ€”alphabet consisting of the SCOT contexts.\r For the fixed
  alphabet size and growing sample size, the Local Asymptotic Normality is proved
  and applied for establishing asymptotically optimal inference.\r We outline what
  obstacles arise for a large SCOT alphabet size and not necessarily vast sample size.\r
  Training SCOT on a large string using clusters of computers and statistical applications
  are described."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: malyutov17a
month: 0
tex_title: "{SCOT} Approximation, Training and Asymptotic Inference"
firstpage: 241
lastpage: 265
page: 241-265
order: 241
cycles: false
author:
- given: Mikhail
  family: Malyutov
- given: Paul
  family: Grosu
date: 2017-05-31
address: 
publisher: PMLR
container-title: Proceedings of the Sixth Workshop on Conformal and Probabilistic
  Prediction and Applications
volume: '60'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 5
  - 31
pdf: http://proceedings.mlr.press/v60/malyutov17a/malyutov17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v60/malyutov17a/malyutov17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
